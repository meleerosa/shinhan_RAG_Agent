import json
import os
import pandas as pd
from tqdm import tqdm
from langchain_openai import ChatOpenAI
from langchain_core.messages import SystemMessage, HumanMessage, AIMessage

# ==========================================
# [ì„¤ì •] ì—ì´ì „íŠ¸ Import
# ==========================================
try:
    from real_agent_final_v4 import app
    print(">>> Agent ë¡œë“œ ì„±ê³µ!")
except ImportError:
    print("Error: ì—ì´ì „íŠ¸ íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
    exit(1)

# ==========================================
# 0. ì„¤ì •
# ==========================================
TEST_DATA_PATH = "/home/wlaud/projects/shinhan/data/test_golden_20.json"
RESULT_CSV_PATH = "/home/wlaud/projects/shinhan/log/evaluation_result.csv"
RESULT_JSON_PATH = "/home/wlaud/projects/shinhan/log/evaluation_detail_log.json"

# ì±„ì ê´€ LLM
judge_llm = ChatOpenAI(model="gpt-4o", temperature=0)

# ==========================================
# 1. í‰ê°€ í•¨ìˆ˜ ì •ì˜
# ==========================================
def calculate_profile_score(expected, extracted):
    """í”„ë¡œí•„ ì¶”ì¶œ ì •í™•ë„ ê³„ì‚°"""
    if not expected: return 1.0
    matched = 0
    total = len(expected)
    for k, v in expected.items():
        extracted_val = str(extracted.get(k, ''))
        expected_val = str(v)
        if expected_val in extracted_val or extracted_val in expected_val:
            matched += 1
        elif k == "age" and extracted_val:
            try:
                if int(float(extracted_val)) == int(v): matched += 1
            except: pass
    return (matched / total) * 100

def check_retrieval_hit(target_product, candidates):
    """ê²€ìƒ‰ ê²°ê³¼ Hit ì—¬ë¶€ í™•ì¸"""
    if not candidates: return False
    for cand in candidates:
        cand_name = cand.get('product_name', '')
        if target_product.replace(" ", "") in cand_name.replace(" ", ""):
            return True
        if cand_name in target_product:
            return True
    return False

def evaluate_response_quality(query, target_product, eval_points, agent_response):
    """LLM ê¸°ë°˜ ë‹µë³€ í’ˆì§ˆ í‰ê°€"""
    prompt = f"""
    [í‰ê°€ ê¸°ì¤€]
    1. ì •í™•ì„±: ìƒí’ˆ({target_product}) ì¶”ì²œ ì—¬ë¶€
    2. í•„ìˆ˜ì‚¬í•­: {eval_points} í¬í•¨ ì—¬ë¶€
    3. ìœ ìš©ì„±: êµ¬ì²´ì  ê°€ì´ë“œ ì œê³µ ì—¬ë¶€
    
    ì§ˆë¬¸: {query}
    ë‹µë³€: {agent_response}
    
    1~10ì  ì‚¬ì´ì˜ ì ìˆ˜(ìˆ«ìë§Œ)ë¥¼ ì¶œë ¥í•˜ì„¸ìš”.
    """
    try:
        res = judge_llm.invoke([SystemMessage(content=prompt)])
        return int(res.content.strip())
    except:
        return 5

# ==========================================
# 2. í…ŒìŠ¤íŠ¸ ì‹¤í–‰ ë£¨í”„ (ë©€í‹°í„´ ì§€ì› & ìƒì„¸ ë¡œê¹…)
# ==========================================
if not os.path.exists(TEST_DATA_PATH):
    print("Error: í…ŒìŠ¤íŠ¸ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")
    exit(1)

with open(TEST_DATA_PATH, 'r') as f:
    test_cases = json.load(f)

results = []
print(f"\n>>> ì´ {len(test_cases)}ê°œ ì¼€ì´ìŠ¤ í‰ê°€ ì‹œì‘ (ë©€í‹°í„´ ìë™ì‘ë‹µ í¬í•¨)...\n")

for case in tqdm(test_cases):
    # 1. ì´ˆê¸° ì§ˆë¬¸ ì„¤ì •
    query = case['query']
    target = case['target_product']
    exp_profile = case['expected_profile']
    
    # 2. ìƒíƒœ ì´ˆê¸°í™”
    history = []
    curr_profile = {}
    curr_ask_count = 0
    
    final_res = ""
    candidates = []
    # [ìˆ˜ì •] ë¡œê·¸ë¥¼ ë‹¨ìˆœ ë¦¬ìŠ¤íŠ¸ê°€ ì•„ë‹ˆë¼ êµ¬ì¡°í™”ëœ ë”•ì…”ë„ˆë¦¬ ë¦¬ìŠ¤íŠ¸ë¡œ ì €ì¥
    log_history = [] 

    user_context_answer = f"ë‚´ ì •ë³´ëŠ” {json.dumps(exp_profile, ensure_ascii=False)} ì•¼."

    for turn in range(3): 
        
        # í„´ë³„ ì‹œì‘ ì‹œê°„ ë“±ì€ ìƒëµí•˜ì§€ë§Œ, ì…ë ¥ ì¿¼ë¦¬ëŠ” ê¸°ë¡
        turn_input_query = query 
        
        inputs = {
            "messages": history, 
            "user_query": query, 
            "profile": curr_profile,
            "ask_count": curr_ask_count,
            "missing_info": [], 
            "candidates": [],
            "thinking_process": [] 
        }
        
        try:
            output = app.invoke(inputs)
            
            final_res = output.get('final_response', "")
            curr_profile = output.get('profile', {})
            curr_ask_count = output.get('ask_count', 0)
            candidates = output.get('candidates', [])
            turn_logs = output.get('thinking_process', [])
            
            # [ìƒì„¸ ë¡œê¹…] í„´ë³„ ìƒì„¸ ì •ë³´ ì €ì¥
            log_history.append({
                "turn": turn + 1,
                "input_query": turn_input_query,
                "agent_response": final_res,
                "extracted_profile": curr_profile.copy(),
                "found_candidates": [c.get('product_name') for c in candidates], # ìƒí’ˆëª…ë§Œ ì¶”ì¶œ
                "internal_logs": turn_logs
            })
            
            has_response_step = any("Response" in log for log in turn_logs)
            
            if has_response_step or candidates:
                break 
            
            history.append(HumanMessage(content=query))
            history.append(AIMessage(content=final_res))
            
            query = user_context_answer 
            
        except Exception as e:
            print(f"Error in Case {case['id']}: {e}")
            final_res = f"Error: {str(e)}"
            log_history.append({"turn": turn+1, "error": str(e)})
            break

    # 3. ìµœì¢… í‰ê°€
    try:
        prof_score = calculate_profile_score(exp_profile, curr_profile)
        is_hit = check_retrieval_hit(target, candidates)
        qual_score = evaluate_response_quality(case['query'], target, case['evaluation_points'], final_res)
        
        results.append({
            "id": case['id'],
            "category": case['category'],
            "initial_query": case['query'],
            "target_product": target,
            "expected_profile": exp_profile,
            
            # í‰ê°€ ì§€í‘œ
            "turns_taken": turn + 1,
            "profile_score": prof_score,
            "retrieval_hit": is_hit,
            "quality_score": qual_score,
            
            # ìµœì¢… ê²°ê³¼ë¬¼
            "final_agent_response": final_res,
            "final_extracted_profile": curr_profile,
            "final_candidates": [c.get('product_name') for c in candidates],
            
            # [Full Log] ì „ì²´ ëŒ€í™” ë° ì‚¬ê³  ê³¼ì •
            "full_interaction_log": log_history 
        })
        
    except Exception as e:
        results.append({"id": case['id'], "error": str(e)})


# ==========================================
# 3. ê²°ê³¼ ì €ì¥
# ==========================================
df = pd.DataFrame(results)

if 'error' not in df.columns:
    df['error'] = None
    
success_df = df[df['error'].isna()].copy()

print("\n" + "="*50)
print("ğŸ“Š [í‰ê°€ ì™„ë£Œ]")

if len(success_df) > 0:
    # CSVëŠ” ìš”ì•½ ì •ë³´ë§Œ ì €ì¥ (ë¡œê·¸ê°€ ë„ˆë¬´ ê¸¸ì–´ì„œ CSV í¬ë§· ê¹¨ì§ ë°©ì§€)
    # full_interaction_log ì»¬ëŸ¼ì€ ì œì™¸í•˜ê±°ë‚˜ ë¬¸ìì—´ë¡œ ë³€í™˜í•´ì„œ ì €ì¥
    csv_df = success_df.drop(columns=['full_interaction_log'])
    csv_df.to_csv(RESULT_CSV_PATH, index=False, encoding='utf-8-sig')
    
    # JSONì—ëŠ” ëª¨ë“  ìƒì„¸ ì •ë³´ í¬í•¨
    success_df.to_json(RESULT_JSON_PATH, orient='records', force_ascii=False, indent=2)
    
    print(f"1. ìš”ì•½ ê²°ê³¼ CSV: {RESULT_CSV_PATH}")
    print(f"2. ìƒì„¸ ë¡œê·¸ JSON: {RESULT_JSON_PATH}")
    print(f"3. í‰ê·  ì ìˆ˜: í’ˆì§ˆ {success_df['quality_score'].mean():.1f}, í”„ë¡œí•„ {success_df['profile_score'].mean():.1f}")
else:
    print("ì‹¤íŒ¨í•œ í…ŒìŠ¤íŠ¸ë§Œ ì¡´ì¬í•©ë‹ˆë‹¤.")
